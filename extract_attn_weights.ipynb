{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import importlib\n",
    "import sys\n",
    "#importlib.reload(sys.modules['fairseq.models.roberta'])\n",
    "from fairseq.models.roberta import RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (v_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): NogradLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): NogradLinear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): NogradLinear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain = RobertaModel.from_pretrained(\n",
    "    '/new_home/zhuocheng/transformer/models/roberta.large',\n",
    "    checkpoint_file='model.pt',\n",
    ").cuda()\n",
    "pretrain.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open('/new_home/zhuocheng/transformer/datasets/glue_data/CoLA/train.tsv', 'r', encoding='utf-8') as fin:\n",
    "    for line in fin:\n",
    "        train_data.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-67\t0\t*\tDid that he played the piano surprise you?\n",
      "\n",
      "Did that he played the piano surprise you?\n",
      "tensor([    0, 20328,    14,    37,   702,     5, 13305,  2755,    47,   116,\n",
      "            2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = random.sample(train_data, 2)[0]\n",
    "print(a)\n",
    "b = a.strip().split('\\t')[-1]\n",
    "print(b)\n",
    "c = pretrain.encode(b).cuda()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open('/new_home/zhuocheng/transformer/datasets/glue_data/CoLA/train.tsv', 'r', encoding='utf-8') as fin:\n",
    "    for line in fin:\n",
    "        train_data.append(line)\n",
    "\n",
    "train_attn = []\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for i in train_data:\n",
    "        sent = i.strip().split('\\t')[-1]\n",
    "        tokens = pretrain.encode(sent).cuda()\n",
    "        attn = pretrain.model.encoder.sentence_encoder.forward_scriptable(tokens.unsqueeze(0))['attn_weights']\n",
    "        attns = torch.stack(attn).squeeze()\n",
    "        train_attn.append(attns.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_data = []\n",
    "with open('/new_home/zhuocheng/transformer/datasets/glue_data/CoLA/dev.tsv', 'r', encoding='utf-8') as fin:\n",
    "    for line in fin:\n",
    "        valid_data.append(line)\n",
    "\n",
    "valid_attn = []\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for i in valid_data:\n",
    "        sent = i.strip().split('\\t')[-1]\n",
    "        tokens = pretrain.encode(sent).cuda()\n",
    "        attn = pretrain.model.encoder.sentence_encoder.forward_scriptable(tokens.unsqueeze(0))['attn_weights']\n",
    "        attns = torch.stack(attn).squeeze()\n",
    "        valid_attn.append(attns.cpu())\n",
    "\n",
    "torch.save(valid_attn, 'CoLA-bin/input0/valid.attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(valid_attn, 'CoLA-bin/input0/valid.attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pretrain.model.encoder.sentence_encoder.forward_scriptable(tokens.unsqueeze(0))['attn_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 9, 9])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everyone claimed that the poison was neutralized. 1\n"
     ]
    }
   ],
   "source": [
    "tokens = random.sample(train_data, 1)[0].strip().split('\\t')\n",
    "sent, target = tokens[-1], tokens[1]\n",
    "print(sent, target)\n",
    "tokens = pretrain.encode(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_attn = pretrain.model.encoder.sentence_encoder.forward_scriptable(tokens.unsqueeze(0))['attn_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "torch.Size([16, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "print(len(pre_attn))\n",
    "print(pre_attn[0].squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 16, 11, 11])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(pre_attn).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_basic(a, b):\n",
    "    res = 0\n",
    "    for i in range(len(a)):\n",
    "        res += wasserstein_distance(a[i], b[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.13261233243331144 1.7378198558153632 1.752287221363657\n",
      "1 0.971153716215498 2.2803670261976055 2.1397048345580743\n",
      "2 0.5554521750420954 1.9973413279025445 1.9882324791034671\n",
      "3 1.0139972043634184 3.139335905933267 2.67003177155075\n",
      "4 0.9823197402890138 3.916214081658086 3.7933111355767752\n",
      "5 1.208766217504394 5.461882534223789 5.362139308712325\n",
      "6 1.3082826459248218 4.597824672018476 4.387153685950609\n",
      "7 1.449606107705581 5.816301996011135 5.8190094251543965\n",
      "8 1.279245120924543 4.294332645427961 4.218336600911167\n",
      "9 1.518400905302722 7.5710816297505845 7.29659077263653\n",
      "10 1.9318525922317527 5.341577278688 4.867779939222645\n",
      "11 1.5309769671502733 6.466154411870743 6.218709221485614\n",
      "12 1.5887899272335362 6.343949524410581 5.85831688233507\n",
      "13 1.5055914101594086 7.032440083455869 6.230813680211622\n",
      "14 2.0266159083046085 6.542214888008692 5.578992886559695\n",
      "15 1.460632795942825 5.309213703531346 5.266570026429668\n",
      "16 1.9567968369653839 6.34970954800327 5.794015902121721\n",
      "17 2.1422264220333385 5.791960026113634 5.806513838396206\n",
      "18 2.0222756398852657 6.166113317002093 6.306484648289924\n",
      "19 1.3796033969382213 6.905225117681307 7.06879049431252\n",
      "20 0.9944001596300994 9.264593354391973 9.451382032460666\n",
      "21 1.1215275075713957 8.905036894936096 9.167339694911197\n",
      "22 3.221489121643731 13.140727470337048 14.218055471599133\n",
      "23 2.078333760339266 16.413496729115522 16.92908390806251\n"
     ]
    }
   ],
   "source": [
    "for layer in range(24):\n",
    "    a = rft_attn[layer].squeeze().tolist()\n",
    "    b = pre_attn[layer].squeeze().tolist()\n",
    "    c = fft_attn[layer].squeeze().tolist()\n",
    "    res_ab = res_ac = res_bc = 0\n",
    "    for i in range(16):\n",
    "        ab = sim_basic(a[i], b[i])\n",
    "        ac = sim_basic(a[i], c[i])\n",
    "        bc = sim_basic(b[i], c[i])\n",
    "        res_ab += ab\n",
    "        res_ac += ac\n",
    "        res_bc += bc\n",
    "    print(layer, res_ab, res_ac, res_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attn = torch.load('CoLA-bin/input0/train.attn')\n",
    "train_mean = []\n",
    "for i in train_attn:\n",
    "    train_mean.append(torch.softmax(i.new(i.shape).fill_(1), dim=-1))\n",
    "torch.save(train_mean, 'CoLA-bin/input0/train.mean')\n",
    "valid_attn = torch.load('CoLA-bin/input0/valid.attn')\n",
    "valid_mean = []\n",
    "for i in valid_attn:\n",
    "    valid_mean.append(torch.softmax(i.new(i.shape).fill_(1), dim=-1))\n",
    "torch.save(valid_mean, 'CoLA-bin/input0/valid.mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attn = torch.load('CoLA-bin/input0/train.attn')\n",
    "train_rand = []\n",
    "for i in train_attn:\n",
    "    train_rand.append(torch.softmax(torch.rand(i.shape), dim=-1))\n",
    "torch.save(train_rand, 'CoLA-bin/input0/train.rand')\n",
    "valid_attn = torch.load('CoLA-bin/input0/valid.attn')\n",
    "valid_rand = []\n",
    "for i in valid_attn:\n",
    "    valid_rand.append(torch.softmax(torch.rand(i.shape), dim=-1))\n",
    "torch.save(valid_rand, 'CoLA-bin/input0/valid.rand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0366, 0.0551, 0.0476,  ..., 0.0373, 0.0478, 0.0431],\n",
       "          [0.0425, 0.0719, 0.0814,  ..., 0.0615, 0.0859, 0.0528],\n",
       "          [0.0475, 0.0719, 0.0353,  ..., 0.0922, 0.0638, 0.0368],\n",
       "          ...,\n",
       "          [0.0442, 0.0411, 0.0826,  ..., 0.0730, 0.0801, 0.0354],\n",
       "          [0.0419, 0.0744, 0.0453,  ..., 0.0808, 0.0382, 0.0715],\n",
       "          [0.0475, 0.0594, 0.0421,  ..., 0.0647, 0.0515, 0.0660]],\n",
       "\n",
       "         [[0.0457, 0.0663, 0.0426,  ..., 0.0733, 0.0553, 0.0418],\n",
       "          [0.0410, 0.0463, 0.0472,  ..., 0.0569, 0.0645, 0.0445],\n",
       "          [0.0593, 0.0407, 0.0806,  ..., 0.0375, 0.0361, 0.0552],\n",
       "          ...,\n",
       "          [0.0377, 0.0818, 0.0353,  ..., 0.0719, 0.0576, 0.0472],\n",
       "          [0.0377, 0.0454, 0.0615,  ..., 0.0401, 0.0472, 0.0515],\n",
       "          [0.0505, 0.0485, 0.0401,  ..., 0.0379, 0.0536, 0.0781]],\n",
       "\n",
       "         [[0.0334, 0.0337, 0.0678,  ..., 0.0513, 0.0438, 0.0788],\n",
       "          [0.0414, 0.0726, 0.0561,  ..., 0.0367, 0.0433, 0.0407],\n",
       "          [0.0690, 0.0382, 0.0566,  ..., 0.0786, 0.0828, 0.0403],\n",
       "          ...,\n",
       "          [0.0380, 0.0354, 0.0524,  ..., 0.0738, 0.0604, 0.0651],\n",
       "          [0.0610, 0.0452, 0.0672,  ..., 0.0353, 0.0425, 0.0572],\n",
       "          [0.0469, 0.0416, 0.0693,  ..., 0.0653, 0.0370, 0.0422]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0468, 0.0513, 0.0826,  ..., 0.0404, 0.0409, 0.0622],\n",
       "          [0.0700, 0.0705, 0.0644,  ..., 0.0670, 0.0411, 0.0353],\n",
       "          [0.0494, 0.0591, 0.0471,  ..., 0.0616, 0.0476, 0.0733],\n",
       "          ...,\n",
       "          [0.0667, 0.0566, 0.0435,  ..., 0.0646, 0.0486, 0.0764],\n",
       "          [0.0365, 0.0545, 0.0514,  ..., 0.0615, 0.0742, 0.0533],\n",
       "          [0.0780, 0.0425, 0.0815,  ..., 0.0635, 0.0600, 0.0346]],\n",
       "\n",
       "         [[0.0583, 0.0414, 0.0863,  ..., 0.0439, 0.0508, 0.0690],\n",
       "          [0.0734, 0.0452, 0.0586,  ..., 0.0494, 0.0492, 0.0600],\n",
       "          [0.0580, 0.0431, 0.0723,  ..., 0.0496, 0.0305, 0.0478],\n",
       "          ...,\n",
       "          [0.0570, 0.0451, 0.0560,  ..., 0.0595, 0.0646, 0.0687],\n",
       "          [0.0385, 0.0468, 0.0628,  ..., 0.0412, 0.0864, 0.0373],\n",
       "          [0.0839, 0.0791, 0.0561,  ..., 0.0376, 0.0493, 0.0427]],\n",
       "\n",
       "         [[0.0582, 0.0859, 0.0429,  ..., 0.0336, 0.0323, 0.0330],\n",
       "          [0.0447, 0.0467, 0.0427,  ..., 0.0568, 0.0872, 0.0408],\n",
       "          [0.0601, 0.0786, 0.0359,  ..., 0.0369, 0.0833, 0.0764],\n",
       "          ...,\n",
       "          [0.0778, 0.0594, 0.0579,  ..., 0.0544, 0.0579, 0.0307],\n",
       "          [0.0908, 0.0657, 0.0657,  ..., 0.0532, 0.0536, 0.0949],\n",
       "          [0.0434, 0.0587, 0.0648,  ..., 0.0356, 0.0799, 0.0624]]],\n",
       "\n",
       "\n",
       "        [[[0.0444, 0.0480, 0.0773,  ..., 0.0558, 0.0658, 0.0379],\n",
       "          [0.0708, 0.0676, 0.0762,  ..., 0.0666, 0.0337, 0.0543],\n",
       "          [0.0649, 0.0403, 0.0684,  ..., 0.0691, 0.0493, 0.0769],\n",
       "          ...,\n",
       "          [0.0760, 0.0663, 0.0368,  ..., 0.0462, 0.0313, 0.0312],\n",
       "          [0.0919, 0.0774, 0.0441,  ..., 0.0680, 0.0572, 0.0404],\n",
       "          [0.0643, 0.0388, 0.0558,  ..., 0.0469, 0.0498, 0.0431]],\n",
       "\n",
       "         [[0.0409, 0.0340, 0.0628,  ..., 0.0348, 0.0805, 0.0584],\n",
       "          [0.0455, 0.0779, 0.0658,  ..., 0.0703, 0.0343, 0.0557],\n",
       "          [0.0635, 0.0871, 0.0879,  ..., 0.0585, 0.0449, 0.0340],\n",
       "          ...,\n",
       "          [0.0326, 0.0679, 0.0434,  ..., 0.0350, 0.0674, 0.0567],\n",
       "          [0.0464, 0.0626, 0.0329,  ..., 0.0543, 0.0447, 0.0508],\n",
       "          [0.0373, 0.0665, 0.0699,  ..., 0.0526, 0.0675, 0.0531]],\n",
       "\n",
       "         [[0.0387, 0.0540, 0.0475,  ..., 0.0772, 0.0816, 0.0605],\n",
       "          [0.0672, 0.0432, 0.0553,  ..., 0.0663, 0.0629, 0.0509],\n",
       "          [0.0334, 0.0633, 0.0637,  ..., 0.0434, 0.0718, 0.0576],\n",
       "          ...,\n",
       "          [0.0364, 0.0573, 0.0413,  ..., 0.0766, 0.0608, 0.0560],\n",
       "          [0.0717, 0.0289, 0.0367,  ..., 0.0685, 0.0739, 0.0665],\n",
       "          [0.0387, 0.0870, 0.0652,  ..., 0.0807, 0.0655, 0.0360]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0606, 0.0515, 0.0691,  ..., 0.0554, 0.0351, 0.0569],\n",
       "          [0.0621, 0.0652, 0.0795,  ..., 0.0569, 0.0354, 0.0599],\n",
       "          [0.0464, 0.0522, 0.0427,  ..., 0.0605, 0.0333, 0.0482],\n",
       "          ...,\n",
       "          [0.0403, 0.0814, 0.0565,  ..., 0.0315, 0.0653, 0.0642],\n",
       "          [0.0575, 0.0339, 0.0796,  ..., 0.0371, 0.0690, 0.0646],\n",
       "          [0.0407, 0.0386, 0.0699,  ..., 0.0422, 0.0784, 0.0446]],\n",
       "\n",
       "         [[0.0720, 0.0427, 0.0883,  ..., 0.0336, 0.0742, 0.0378],\n",
       "          [0.0381, 0.0634, 0.0497,  ..., 0.0545, 0.0531, 0.0502],\n",
       "          [0.0602, 0.0796, 0.0337,  ..., 0.0663, 0.0352, 0.0450],\n",
       "          ...,\n",
       "          [0.0740, 0.0601, 0.0500,  ..., 0.0531, 0.0735, 0.0332],\n",
       "          [0.0571, 0.0442, 0.0398,  ..., 0.0704, 0.0624, 0.0280],\n",
       "          [0.0401, 0.0401, 0.0543,  ..., 0.0430, 0.0856, 0.0441]],\n",
       "\n",
       "         [[0.0357, 0.0591, 0.0336,  ..., 0.0617, 0.0846, 0.0522],\n",
       "          [0.0413, 0.0481, 0.0781,  ..., 0.0342, 0.0671, 0.0352],\n",
       "          [0.0621, 0.0340, 0.0800,  ..., 0.0505, 0.0805, 0.0577],\n",
       "          ...,\n",
       "          [0.0356, 0.0472, 0.0552,  ..., 0.0708, 0.0384, 0.0538],\n",
       "          [0.0529, 0.0351, 0.0482,  ..., 0.0652, 0.0631, 0.0588],\n",
       "          [0.0628, 0.0507, 0.0556,  ..., 0.0773, 0.0778, 0.0481]]],\n",
       "\n",
       "\n",
       "        [[[0.0364, 0.0547, 0.0404,  ..., 0.0461, 0.0563, 0.0416],\n",
       "          [0.0614, 0.0421, 0.0877,  ..., 0.0663, 0.0374, 0.0343],\n",
       "          [0.0509, 0.0816, 0.0511,  ..., 0.0724, 0.0822, 0.0603],\n",
       "          ...,\n",
       "          [0.0434, 0.0740, 0.0595,  ..., 0.0798, 0.0493, 0.0617],\n",
       "          [0.0504, 0.0402, 0.0431,  ..., 0.0630, 0.0879, 0.0543],\n",
       "          [0.0753, 0.0662, 0.0521,  ..., 0.0404, 0.0701, 0.0648]],\n",
       "\n",
       "         [[0.0705, 0.0531, 0.0749,  ..., 0.0779, 0.0651, 0.0632],\n",
       "          [0.0727, 0.0413, 0.0710,  ..., 0.0375, 0.0484, 0.0419],\n",
       "          [0.0421, 0.0312, 0.0660,  ..., 0.0525, 0.0386, 0.0658],\n",
       "          ...,\n",
       "          [0.0419, 0.0541, 0.0677,  ..., 0.0332, 0.0813, 0.0470],\n",
       "          [0.0553, 0.0638, 0.0339,  ..., 0.0481, 0.0743, 0.0698],\n",
       "          [0.0429, 0.0708, 0.0515,  ..., 0.0404, 0.0458, 0.0461]],\n",
       "\n",
       "         [[0.0843, 0.0817, 0.0335,  ..., 0.0633, 0.0672, 0.0343],\n",
       "          [0.0720, 0.0423, 0.0466,  ..., 0.0478, 0.0370, 0.0778],\n",
       "          [0.0771, 0.0605, 0.0687,  ..., 0.0442, 0.0545, 0.0651],\n",
       "          ...,\n",
       "          [0.0327, 0.0631, 0.0787,  ..., 0.0330, 0.0646, 0.0425],\n",
       "          [0.0651, 0.0550, 0.0392,  ..., 0.0474, 0.0346, 0.0458],\n",
       "          [0.0587, 0.0472, 0.0668,  ..., 0.0433, 0.0401, 0.0553]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0626, 0.0545, 0.0543,  ..., 0.0497, 0.0525, 0.0358],\n",
       "          [0.0355, 0.0460, 0.0775,  ..., 0.0426, 0.0398, 0.0469],\n",
       "          [0.0412, 0.0372, 0.0409,  ..., 0.0793, 0.0782, 0.0686],\n",
       "          ...,\n",
       "          [0.0328, 0.0356, 0.0402,  ..., 0.0786, 0.0798, 0.0341],\n",
       "          [0.0674, 0.0610, 0.0726,  ..., 0.0684, 0.0320, 0.0554],\n",
       "          [0.0505, 0.0805, 0.0415,  ..., 0.0578, 0.0446, 0.0489]],\n",
       "\n",
       "         [[0.0888, 0.0388, 0.0628,  ..., 0.0502, 0.0394, 0.0726],\n",
       "          [0.0554, 0.0579, 0.0491,  ..., 0.0584, 0.0500, 0.0781],\n",
       "          [0.0367, 0.0461, 0.0641,  ..., 0.0382, 0.0303, 0.0787],\n",
       "          ...,\n",
       "          [0.0530, 0.0803, 0.0356,  ..., 0.0847, 0.0363, 0.0683],\n",
       "          [0.0534, 0.0359, 0.0879,  ..., 0.0491, 0.0513, 0.0520],\n",
       "          [0.0670, 0.0360, 0.0692,  ..., 0.0414, 0.0794, 0.0553]],\n",
       "\n",
       "         [[0.0567, 0.0350, 0.0804,  ..., 0.0590, 0.0427, 0.0363],\n",
       "          [0.0618, 0.0353, 0.0327,  ..., 0.0403, 0.0446, 0.0813],\n",
       "          [0.0702, 0.0516, 0.0675,  ..., 0.0456, 0.0692, 0.0816],\n",
       "          ...,\n",
       "          [0.0498, 0.0309, 0.0495,  ..., 0.0428, 0.0678, 0.0410],\n",
       "          [0.0472, 0.0443, 0.0451,  ..., 0.0388, 0.0340, 0.0538],\n",
       "          [0.0474, 0.0441, 0.0355,  ..., 0.0495, 0.0376, 0.0823]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.0431, 0.0510, 0.0356,  ..., 0.0667, 0.0866, 0.0478],\n",
       "          [0.0602, 0.0507, 0.0398,  ..., 0.0417, 0.0919, 0.0683],\n",
       "          [0.0488, 0.0552, 0.0682,  ..., 0.0587, 0.0451, 0.0468],\n",
       "          ...,\n",
       "          [0.0677, 0.0401, 0.0515,  ..., 0.0554, 0.0532, 0.0869],\n",
       "          [0.0823, 0.0805, 0.0385,  ..., 0.0397, 0.0364, 0.0398],\n",
       "          [0.0785, 0.0667, 0.0422,  ..., 0.0543, 0.0450, 0.0755]],\n",
       "\n",
       "         [[0.0831, 0.0457, 0.0387,  ..., 0.0511, 0.0524, 0.0740],\n",
       "          [0.0813, 0.0524, 0.0534,  ..., 0.0448, 0.0640, 0.0366],\n",
       "          [0.0437, 0.0647, 0.0370,  ..., 0.0745, 0.0505, 0.0412],\n",
       "          ...,\n",
       "          [0.0307, 0.0777, 0.0807,  ..., 0.0533, 0.0805, 0.0576],\n",
       "          [0.0332, 0.0582, 0.0738,  ..., 0.0356, 0.0439, 0.0703],\n",
       "          [0.0389, 0.0691, 0.0715,  ..., 0.0596, 0.0769, 0.0606]],\n",
       "\n",
       "         [[0.0337, 0.0517, 0.0482,  ..., 0.0619, 0.0464, 0.0362],\n",
       "          [0.0800, 0.0381, 0.0755,  ..., 0.0420, 0.0447, 0.0468],\n",
       "          [0.0759, 0.0793, 0.0835,  ..., 0.0509, 0.0402, 0.0589],\n",
       "          ...,\n",
       "          [0.0639, 0.0655, 0.0625,  ..., 0.0291, 0.0660, 0.0698],\n",
       "          [0.0606, 0.0727, 0.0689,  ..., 0.0581, 0.0513, 0.0403],\n",
       "          [0.0466, 0.0464, 0.0628,  ..., 0.0410, 0.0705, 0.0528]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0416, 0.0961, 0.0513,  ..., 0.0468, 0.0418, 0.0526],\n",
       "          [0.0447, 0.0406, 0.0857,  ..., 0.0376, 0.0745, 0.0525],\n",
       "          [0.0602, 0.0750, 0.0685,  ..., 0.0496, 0.0461, 0.0390],\n",
       "          ...,\n",
       "          [0.0745, 0.0683, 0.0620,  ..., 0.0653, 0.0783, 0.0368],\n",
       "          [0.0689, 0.0341, 0.0365,  ..., 0.0491, 0.0516, 0.0469],\n",
       "          [0.0361, 0.0891, 0.0340,  ..., 0.0612, 0.0439, 0.0564]],\n",
       "\n",
       "         [[0.0443, 0.0394, 0.0483,  ..., 0.0412, 0.0585, 0.0719],\n",
       "          [0.0367, 0.0627, 0.0391,  ..., 0.0333, 0.0759, 0.0489],\n",
       "          [0.0751, 0.0467, 0.0742,  ..., 0.0560, 0.0367, 0.0425],\n",
       "          ...,\n",
       "          [0.0530, 0.0561, 0.0407,  ..., 0.0592, 0.0366, 0.0523],\n",
       "          [0.0747, 0.0508, 0.0336,  ..., 0.0610, 0.0765, 0.0501],\n",
       "          [0.0361, 0.0374, 0.0445,  ..., 0.0536, 0.0458, 0.0869]],\n",
       "\n",
       "         [[0.0765, 0.0407, 0.0529,  ..., 0.0590, 0.0454, 0.0410],\n",
       "          [0.0456, 0.0544, 0.0834,  ..., 0.0362, 0.0523, 0.0399],\n",
       "          [0.0334, 0.0858, 0.0401,  ..., 0.0591, 0.0525, 0.0527],\n",
       "          ...,\n",
       "          [0.0416, 0.0334, 0.0363,  ..., 0.0481, 0.0556, 0.0762],\n",
       "          [0.0639, 0.0376, 0.0755,  ..., 0.0414, 0.0593, 0.0821],\n",
       "          [0.0406, 0.0427, 0.0652,  ..., 0.0720, 0.0367, 0.0525]]],\n",
       "\n",
       "\n",
       "        [[[0.0520, 0.0489, 0.0815,  ..., 0.0433, 0.0728, 0.0796],\n",
       "          [0.0712, 0.0806, 0.0438,  ..., 0.0454, 0.0436, 0.0777],\n",
       "          [0.0740, 0.0782, 0.0518,  ..., 0.0366, 0.0487, 0.0506],\n",
       "          ...,\n",
       "          [0.0397, 0.0661, 0.0764,  ..., 0.0594, 0.0574, 0.0511],\n",
       "          [0.0349, 0.0480, 0.0686,  ..., 0.0376, 0.0924, 0.0351],\n",
       "          [0.0539, 0.0779, 0.0753,  ..., 0.0753, 0.0669, 0.0323]],\n",
       "\n",
       "         [[0.0479, 0.0412, 0.0737,  ..., 0.0843, 0.0590, 0.0353],\n",
       "          [0.0418, 0.0445, 0.0583,  ..., 0.0695, 0.0659, 0.0499],\n",
       "          [0.0615, 0.0422, 0.0511,  ..., 0.0933, 0.0688, 0.0875],\n",
       "          ...,\n",
       "          [0.0353, 0.0392, 0.0479,  ..., 0.0725, 0.0612, 0.0446],\n",
       "          [0.0658, 0.0569, 0.0572,  ..., 0.0474, 0.0681, 0.0670],\n",
       "          [0.0385, 0.0454, 0.0966,  ..., 0.0415, 0.0384, 0.0382]],\n",
       "\n",
       "         [[0.0412, 0.0587, 0.0746,  ..., 0.0891, 0.0548, 0.0844],\n",
       "          [0.0362, 0.0810, 0.0417,  ..., 0.0619, 0.0756, 0.0346],\n",
       "          [0.0713, 0.0407, 0.0697,  ..., 0.0395, 0.0830, 0.0594],\n",
       "          ...,\n",
       "          [0.0521, 0.0468, 0.0478,  ..., 0.0398, 0.0869, 0.0616],\n",
       "          [0.0615, 0.0738, 0.0503,  ..., 0.0411, 0.0403, 0.0734],\n",
       "          [0.0663, 0.0631, 0.0475,  ..., 0.0427, 0.0450, 0.0451]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0447, 0.0519, 0.0908,  ..., 0.0577, 0.0415, 0.0408],\n",
       "          [0.0382, 0.0522, 0.0779,  ..., 0.0714, 0.0549, 0.0466],\n",
       "          [0.0523, 0.0711, 0.0688,  ..., 0.0815, 0.0681, 0.0680],\n",
       "          ...,\n",
       "          [0.0772, 0.0799, 0.0533,  ..., 0.0318, 0.0437, 0.0790],\n",
       "          [0.0421, 0.0419, 0.0515,  ..., 0.0646, 0.0624, 0.0420],\n",
       "          [0.0529, 0.0361, 0.0344,  ..., 0.0750, 0.0615, 0.0527]],\n",
       "\n",
       "         [[0.0862, 0.0746, 0.0681,  ..., 0.0518, 0.0487, 0.0370],\n",
       "          [0.0625, 0.0563, 0.0792,  ..., 0.0482, 0.0889, 0.0441],\n",
       "          [0.0740, 0.0699, 0.0703,  ..., 0.0697, 0.0597, 0.0737],\n",
       "          ...,\n",
       "          [0.0706, 0.0775, 0.0375,  ..., 0.0678, 0.0838, 0.0323],\n",
       "          [0.0412, 0.0400, 0.0393,  ..., 0.0738, 0.0542, 0.0815],\n",
       "          [0.0448, 0.0528, 0.0546,  ..., 0.0561, 0.0623, 0.0394]],\n",
       "\n",
       "         [[0.0413, 0.0412, 0.0574,  ..., 0.0540, 0.0411, 0.0786],\n",
       "          [0.0468, 0.0409, 0.0635,  ..., 0.0743, 0.0771, 0.0296],\n",
       "          [0.0615, 0.0511, 0.0700,  ..., 0.0408, 0.0789, 0.0699],\n",
       "          ...,\n",
       "          [0.0368, 0.0790, 0.0621,  ..., 0.0714, 0.0441, 0.0369],\n",
       "          [0.0488, 0.0424, 0.0480,  ..., 0.0456, 0.0393, 0.0724],\n",
       "          [0.0591, 0.0675, 0.0653,  ..., 0.0725, 0.0369, 0.0750]]],\n",
       "\n",
       "\n",
       "        [[[0.0701, 0.0672, 0.0565,  ..., 0.0597, 0.0664, 0.0428],\n",
       "          [0.0536, 0.0836, 0.0614,  ..., 0.0471, 0.0502, 0.0521],\n",
       "          [0.0720, 0.0659, 0.0820,  ..., 0.0681, 0.0553, 0.0689],\n",
       "          ...,\n",
       "          [0.0524, 0.0820, 0.0595,  ..., 0.0693, 0.0340, 0.0394],\n",
       "          [0.0434, 0.0508, 0.0375,  ..., 0.0734, 0.0702, 0.0484],\n",
       "          [0.0851, 0.0425, 0.0706,  ..., 0.0653, 0.0495, 0.0419]],\n",
       "\n",
       "         [[0.0566, 0.0470, 0.0612,  ..., 0.0704, 0.0686, 0.0468],\n",
       "          [0.0715, 0.0673, 0.0508,  ..., 0.0580, 0.0437, 0.0601],\n",
       "          [0.0602, 0.0432, 0.0365,  ..., 0.0401, 0.0532, 0.0506],\n",
       "          ...,\n",
       "          [0.0599, 0.0471, 0.0852,  ..., 0.0722, 0.0754, 0.0744],\n",
       "          [0.0751, 0.0667, 0.0740,  ..., 0.0444, 0.0692, 0.0543],\n",
       "          [0.0373, 0.0679, 0.0444,  ..., 0.0624, 0.0604, 0.0829]],\n",
       "\n",
       "         [[0.0397, 0.0695, 0.0624,  ..., 0.0529, 0.0791, 0.0835],\n",
       "          [0.0777, 0.0621, 0.0612,  ..., 0.0413, 0.0469, 0.0741],\n",
       "          [0.0551, 0.0698, 0.0571,  ..., 0.0402, 0.0408, 0.0813],\n",
       "          ...,\n",
       "          [0.0811, 0.0689, 0.0657,  ..., 0.0744, 0.0696, 0.0805],\n",
       "          [0.0729, 0.0526, 0.0880,  ..., 0.0438, 0.0617, 0.0365],\n",
       "          [0.0378, 0.0449, 0.0695,  ..., 0.0381, 0.0824, 0.0494]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0547, 0.0463, 0.0450,  ..., 0.0452, 0.0456, 0.0584],\n",
       "          [0.0651, 0.0598, 0.0426,  ..., 0.0746, 0.0535, 0.0452],\n",
       "          [0.0691, 0.0797, 0.0649,  ..., 0.0493, 0.0579, 0.0315],\n",
       "          ...,\n",
       "          [0.0427, 0.0386, 0.0358,  ..., 0.0701, 0.0623, 0.0727],\n",
       "          [0.0622, 0.0320, 0.0655,  ..., 0.0744, 0.0373, 0.0540],\n",
       "          [0.0362, 0.0630, 0.0322,  ..., 0.0554, 0.0382, 0.0778]],\n",
       "\n",
       "         [[0.0429, 0.0571, 0.0411,  ..., 0.0392, 0.0446, 0.0545],\n",
       "          [0.0380, 0.0366, 0.0489,  ..., 0.0667, 0.0916, 0.0846],\n",
       "          [0.0814, 0.0540, 0.0453,  ..., 0.0680, 0.0483, 0.0505],\n",
       "          ...,\n",
       "          [0.0539, 0.0552, 0.0684,  ..., 0.0663, 0.0447, 0.0511],\n",
       "          [0.0726, 0.0705, 0.0357,  ..., 0.0557, 0.0587, 0.0544],\n",
       "          [0.0483, 0.0397, 0.0573,  ..., 0.0554, 0.0536, 0.0760]],\n",
       "\n",
       "         [[0.0365, 0.0434, 0.0734,  ..., 0.0629, 0.0546, 0.0772],\n",
       "          [0.0492, 0.0593, 0.0632,  ..., 0.0313, 0.0696, 0.0489],\n",
       "          [0.0825, 0.0384, 0.0637,  ..., 0.0352, 0.0595, 0.0403],\n",
       "          ...,\n",
       "          [0.0418, 0.0438, 0.0504,  ..., 0.0472, 0.0453, 0.0419],\n",
       "          [0.0744, 0.0716, 0.0695,  ..., 0.0713, 0.0327, 0.0766],\n",
       "          [0.0399, 0.0727, 0.0880,  ..., 0.0733, 0.0790, 0.0641]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rand[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
