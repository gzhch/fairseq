2021-09-14 05:30:07 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2200, 'batch_size': 16, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2200, 'batch_size_valid': 16, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 30, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'train_bias': False, 'rft_rate': 0.0, 'freeze_emb': True, 'freeze_norm': False, 'l1_regularization': 0.01}, 'checkpoint': {'_name': None, 'save_dir': 'tmp/out', 'restore_file': '../transformer/models/roberta.large/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': 8, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_large', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, add_prev_output_tokens=False, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='roberta_large', attention_dropout=0.1, azureml_logging=False, batch_size=16, batch_size_valid=16, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='sentence_prediction', curriculum=0, data='../FastBERT/examples/roberta/glue/MRPC-bin/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, fp32_reduce_scatter=False, freeze_emb=True, freeze_norm=False, ft_layer=[], gen_subset='test', grad_dropout=False, graded_rft='linear', heartbeat_timeout=-1, ignore_unused_valid_subsets=False, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, l1_regularization=0.01, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lora=0, lr=[0.0001], lr_scheduler='polynomial_decay', mask_type=2.0, max_epoch=30, max_positions=512, max_source_positions=512, max_tokens=2200, max_tokens_valid=2200, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_overrides='{}', model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_shuffle=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_classes=2, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, path=None, patience=8, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, post_process=None, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, quiet=False, random_ft=0.0, regression_target=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='../transformer/models/roberta.large/model.pt', results_path=None, rft_rate=0, save_dir='tmp/out', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, separator_token=2, shard_id=0, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, task='sentence_prediction', tensorboard_logdir=None, threshold_loss_scale=1.0, tokenizer=None, total_num_update='6900', tpu=False, train_bias=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=460, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='sentence_prediction', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, add_prev_output_tokens=False, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='roberta_large', attention_dropout=0.1, azureml_logging=False, batch_size=16, batch_size_valid=16, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='sentence_prediction', curriculum=0, data='../FastBERT/examples/roberta/glue/MRPC-bin/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, fp32_reduce_scatter=False, freeze_emb=True, freeze_norm=False, ft_layer=[], gen_subset='test', grad_dropout=False, graded_rft='linear', heartbeat_timeout=-1, ignore_unused_valid_subsets=False, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, l1_regularization=0.01, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lora=0, lr=[0.0001], lr_scheduler='polynomial_decay', mask_type=2.0, max_epoch=30, max_positions=512, max_source_positions=512, max_tokens=2200, max_tokens_valid=2200, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_overrides='{}', model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_shuffle=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_classes=2, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, path=None, patience=8, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, post_process=None, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, quiet=False, random_ft=0.0, regression_target=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='../transformer/models/roberta.large/model.pt', results_path=None, rft_rate=0, save_dir='tmp/out', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, separator_token=2, shard_id=0, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, task='sentence_prediction', tensorboard_logdir=None, threshold_loss_scale=1.0, tokenizer=None, total_num_update='6900', tpu=False, train_bias=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=460, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': Namespace(_name='sentence_prediction', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, add_prev_output_tokens=False, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='roberta_large', attention_dropout=0.1, azureml_logging=False, batch_size=16, batch_size_valid=16, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='sentence_prediction', curriculum=0, data='../FastBERT/examples/roberta/glue/MRPC-bin/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, fp32_reduce_scatter=False, freeze_emb=True, freeze_norm=False, ft_layer=[], gen_subset='test', grad_dropout=False, graded_rft='linear', heartbeat_timeout=-1, ignore_unused_valid_subsets=False, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, l1_regularization=0.01, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lora=0, lr=[0.0001], lr_scheduler='polynomial_decay', mask_type=2.0, max_epoch=30, max_positions=512, max_source_positions=512, max_tokens=2200, max_tokens_valid=2200, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_overrides='{}', model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_shuffle=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_classes=2, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, path=None, patience=8, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, post_process=None, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, quiet=False, random_ft=0.0, regression_target=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='../transformer/models/roberta.large/model.pt', results_path=None, rft_rate=0, save_dir='tmp/out', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, separator_token=2, shard_id=0, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, task='sentence_prediction', tensorboard_logdir=None, threshold_loss_scale=1.0, tokenizer=None, total_num_update='6900', tpu=False, train_bias=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=460, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 460, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 6900.0, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'simul_type': None}
2021-09-14 05:30:07 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types
2021-09-14 05:30:07 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 7 types
2021-09-14 05:30:15 | INFO | fairseq_cli.train | RobertaModel(
  (encoder): RobertaEncoder(
    (sentence_encoder): TransformerEncoder(
      (dropout_module): FairseqDropout()
      (embed_tokens): Embedding(50265, 1024, padding_idx=1)
      (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)
      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (12): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (13): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (14): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (15): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (16): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (17): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (18): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (19): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (20): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (21): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (22): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (23): TransformerEncoderLayer(
          (l1_penalty): L1Loss()
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): L1Linear(in_features=1024, out_features=1024, bias=True)
          )
          (self_attn_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): L1Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): L1Linear(in_features=4096, out_features=1024, bias=True)
          (final_layer_norm): L1LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (lm_head): RobertaLMHead(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
  (classification_heads): ModuleDict(
    (sentence_classification_head): RobertaClassificationHead(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (out_proj): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
2021-09-14 05:30:15 | INFO | fairseq_cli.train | task: SentencePredictionTask
2021-09-14 05:30:15 | INFO | fairseq_cli.train | model: RobertaModel
2021-09-14 05:30:15 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion
2021-09-14 05:30:15 | INFO | fairseq_cli.train | num. shared model params: 658,772,059 (num. trained: 356,462,683)
2021-09-14 05:30:15 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2021-09-14 05:30:15 | INFO | fairseq.data.data_utils | loaded 408 examples from: ../FastBERT/examples/roberta/glue/MRPC-bin/input0/valid
2021-09-14 05:30:15 | INFO | fairseq.data.data_utils | loaded 408 examples from: ../FastBERT/examples/roberta/glue/MRPC-bin/input1/valid
2021-09-14 05:30:15 | INFO | fairseq.data.data_utils | loaded 408 examples from: ../FastBERT/examples/roberta/glue/MRPC-bin/label/valid
2021-09-14 05:30:15 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 408
2021-09-14 05:30:19 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight
2021-09-14 05:30:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-09-14 05:30:19 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB                    
2021-09-14 05:30:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-09-14 05:30:19 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-09-14 05:30:19 | INFO | fairseq_cli.train | max tokens per device = 2200 and max sentences per device = 16
2021-09-14 05:30:19 | INFO | fairseq.trainer | Preparing to load checkpoint ../transformer/models/roberta.large/model.pt
2021-09-14 05:30:21 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight
2021-09-14 05:30:21 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias
2021-09-14 05:30:21 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight
2021-09-14 05:30:21 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias
2021-09-14 05:30:21 | INFO | fairseq.optim.adam | using FusedAdam
2021-09-14 05:30:21 | INFO | fairseq.trainer | Loaded checkpoint ../transformer/models/roberta.large/model.pt (epoch 1 @ 0 updates)
2021-09-14 05:30:21 | INFO | fairseq.trainer | loading train data for epoch 1
2021-09-14 05:30:21 | INFO | fairseq.data.data_utils | loaded 3,668 examples from: ../FastBERT/examples/roberta/glue/MRPC-bin/input0/train
2021-09-14 05:30:21 | INFO | fairseq.data.data_utils | loaded 3,668 examples from: ../FastBERT/examples/roberta/glue/MRPC-bin/input1/train
2021-09-14 05:30:21 | INFO | fairseq.data.data_utils | loaded 3,668 examples from: ../FastBERT/examples/roberta/glue/MRPC-bin/label/train
2021-09-14 05:30:21 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 3668
2021-09-14 05:30:22 | INFO | fairseq.trainer | begin training epoch 1
2021-09-14 05:30:22 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:30:39 | INFO | train_inner | epoch 001:    100 / 230 loss=0.98, nll_loss=0.018, accuracy=60.9, sample_size=16, f1=0.687227, mcc=0.0125178, acc_f1=0.648613, wps=4824.1, ups=5.59, wpb=862.6, bsz=16, num_updates=100, lr=2.17391e-05, gnorm=8.219, loss_scale=4, train_wall=18, gb_free=8.3, wall=21
2021-09-14 05:30:57 | INFO | train_inner | epoch 001:    200 / 230 loss=0.95, nll_loss=0.017, accuracy=67.8, sample_size=15.88, f1=0.800496, mcc=0, acc_f1=0.738686, wps=4801.6, ups=5.57, wpb=861.7, bsz=15.9, num_updates=200, lr=4.34783e-05, gnorm=15.19, loss_scale=8, train_wall=18, gb_free=8.5, wall=38
2021-09-14 05:31:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:31:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.933 | nll_loss 0.017 | accuracy 68.4 | sample_size 15.6923 | f1 0.803658 | mcc 0 | acc_f1 0.741973 | wps 21277 | wpb 851.3 | bsz 15.7 | num_updates 230
2021-09-14 05:31:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 230 updates
2021-09-14 05:31:04 | INFO | fairseq.trainer | Saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:31:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint tmp/out/checkpoint_best.pt (epoch 1 @ 230 updates, score 68.4) (writing took 10.637756407260895 seconds)
2021-09-14 05:31:15 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-09-14 05:31:15 | INFO | train | epoch 001 | loss 0.963 | nll_loss 0.018 | accuracy 64.6 | sample_size 15.9478 | f1 0.749229 | mcc 0.00544251 | acc_f1 0.697305 | wps 3743.1 | ups 4.34 | wpb 862.5 | bsz 15.9 | num_updates 230 | lr 5e-05 | gnorm 15.828 | loss_scale 8 | train_wall 41 | gb_free 8.3 | wall 56
2021-09-14 05:31:15 | INFO | fairseq.trainer | begin training epoch 2
2021-09-14 05:31:15 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:31:27 | INFO | train_inner | epoch 002:     70 / 230 loss=0.936, nll_loss=0.017, accuracy=66.6, sample_size=16, f1=0.792541, mcc=0, acc_f1=0.729083, wps=2891.1, ups=3.34, wpb=865.5, bsz=16, num_updates=300, lr=6.52174e-05, gnorm=17.945, loss_scale=16, train_wall=18, gb_free=8.3, wall=68
2021-09-14 05:31:45 | INFO | train_inner | epoch 002:    170 / 230 loss=0.929, nll_loss=0.017, accuracy=68.1, sample_size=16, f1=0.802897, mcc=0, acc_f1=0.741761, wps=4828.4, ups=5.57, wpb=867.2, bsz=16, num_updates=400, lr=8.69565e-05, gnorm=8.434, loss_scale=32, train_wall=18, gb_free=8.7, wall=86
2021-09-14 05:31:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-09-14 05:31:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:31:57 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.915 | nll_loss 0.017 | accuracy 68.4 | sample_size 15.6923 | f1 0.803658 | mcc 0 | acc_f1 0.741973 | wps 21380.9 | wpb 851.3 | bsz 15.7 | num_updates 459 | best_accuracy 68.4
2021-09-14 05:31:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 459 updates
2021-09-14 05:31:57 | INFO | fairseq.trainer | Saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:32:07 | INFO | fairseq.trainer | Finished saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint tmp/out/checkpoint_best.pt (epoch 2 @ 459 updates, score 68.4) (writing took 10.007908083498478 seconds)
2021-09-14 05:32:07 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-09-14 05:32:07 | INFO | train | epoch 002 | loss 0.927 | nll_loss 0.017 | accuracy 67.4 | sample_size 15.9476 | f1 0.798316 | mcc 0.00126909 | acc_f1 0.735676 | wps 3763.9 | ups 4.37 | wpb 862.2 | bsz 15.9 | num_updates 459 | lr 9.97826e-05 | gnorm 8.513 | loss_scale 16 | train_wall 41 | gb_free 8.7 | wall 108
2021-09-14 05:32:07 | INFO | fairseq.trainer | begin training epoch 3
2021-09-14 05:32:07 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:32:14 | INFO | train_inner | epoch 003:     41 / 230 loss=0.932, nll_loss=0.017, accuracy=66.9, sample_size=15.76, f1=0.792618, mcc=-0.00472546, acc_f1=0.729434, wps=2913.2, ups=3.43, wpb=849.7, bsz=15.8, num_updates=500, lr=9.93789e-05, gnorm=13.75, loss_scale=16, train_wall=18, gb_free=8.4, wall=116
2021-09-14 05:32:33 | INFO | train_inner | epoch 003:    141 / 230 loss=0.93, nll_loss=0.017, accuracy=66.3, sample_size=16, f1=0.788974, mcc=0.00354854, acc_f1=0.726362, wps=4783.3, ups=5.53, wpb=865.4, bsz=16, num_updates=600, lr=9.78261e-05, gnorm=12.633, loss_scale=32, train_wall=18, gb_free=7.7, wall=134
2021-09-14 05:32:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:32:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.89 | nll_loss 0.016 | accuracy 68.9 | sample_size 15.6923 | f1 0.805306 | mcc 0.0341686 | acc_f1 0.745201 | wps 21300.9 | wpb 851.3 | bsz 15.7 | num_updates 689 | best_accuracy 68.9
2021-09-14 05:32:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 689 updates
2021-09-14 05:32:50 | INFO | fairseq.trainer | Saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:33:00 | INFO | fairseq.trainer | Finished saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:33:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint tmp/out/checkpoint_best.pt (epoch 3 @ 689 updates, score 68.9) (writing took 10.076737377792597 seconds)
2021-09-14 05:33:00 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-09-14 05:33:00 | INFO | train | epoch 003 | loss 0.931 | nll_loss 0.017 | accuracy 66.9 | sample_size 15.9478 | f1 0.793124 | mcc 0.00511746 | acc_f1 0.730801 | wps 3764.3 | ups 4.36 | wpb 862.5 | bsz 15.9 | num_updates 689 | lr 9.64441e-05 | gnorm 12.911 | loss_scale 32 | train_wall 41 | gb_free 8.3 | wall 161
2021-09-14 05:33:00 | INFO | fairseq.trainer | begin training epoch 4
2021-09-14 05:33:00 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:33:02 | INFO | train_inner | epoch 004:     11 / 230 loss=0.928, nll_loss=0.017, accuracy=67.1, sample_size=16, f1=0.795547, mcc=0.0140354, acc_f1=0.733398, wps=2962.5, ups=3.43, wpb=863.5, bsz=16, num_updates=700, lr=9.62733e-05, gnorm=15.467, loss_scale=32, train_wall=18, gb_free=8.4, wall=163
2021-09-14 05:33:20 | INFO | train_inner | epoch 004:    111 / 230 loss=0.906, nll_loss=0.017, accuracy=67.9, sample_size=15.88, f1=0.776436, mcc=0.0467858, acc_f1=0.726968, wps=4690, ups=5.48, wpb=855.3, bsz=15.9, num_updates=800, lr=9.47205e-05, gnorm=9.892, loss_scale=64, train_wall=18, gb_free=8.3, wall=181
2021-09-14 05:33:38 | INFO | train_inner | epoch 004:    211 / 230 loss=0.876, nll_loss=0.016, accuracy=69, sample_size=16, f1=0.79887, mcc=0.124692, acc_f1=0.744435, wps=4861.8, ups=5.6, wpb=867.5, bsz=16, num_updates=900, lr=9.31677e-05, gnorm=13.249, loss_scale=128, train_wall=18, gb_free=8.3, wall=199
2021-09-14 05:33:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:33:42 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.739 | nll_loss 0.014 | accuracy 71.3 | sample_size 15.6923 | f1 0.809159 | mcc 0.251837 | acc_f1 0.759147 | wps 21430.4 | wpb 851.3 | bsz 15.7 | num_updates 919 | best_accuracy 71.3
2021-09-14 05:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 919 updates
2021-09-14 05:33:42 | INFO | fairseq.trainer | Saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:33:52 | INFO | fairseq.trainer | Finished saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:33:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint tmp/out/checkpoint_best.pt (epoch 4 @ 919 updates, score 71.3) (writing took 10.195635795593262 seconds)
2021-09-14 05:33:52 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-09-14 05:33:52 | INFO | train | epoch 004 | loss 0.896 | nll_loss 0.017 | accuracy 67.8 | sample_size 15.9478 | f1 0.784034 | mcc 0.0769375 | acc_f1 0.730604 | wps 3756 | ups 4.35 | wpb 862.5 | bsz 15.9 | num_updates 919 | lr 9.28727e-05 | gnorm 13.326 | loss_scale 128 | train_wall 41 | gb_free 7.9 | wall 214
2021-09-14 05:33:52 | INFO | fairseq.trainer | begin training epoch 5
2021-09-14 05:33:52 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:34:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2021-09-14 05:34:08 | INFO | train_inner | epoch 005:     82 / 230 loss=0.86, nll_loss=0.016, accuracy=68.3, sample_size=15.88, f1=0.790949, mcc=0.118631, acc_f1=0.736412, wps=2887.7, ups=3.35, wpb=861.3, bsz=15.9, num_updates=1000, lr=9.16149e-05, gnorm=14.421, loss_scale=128, train_wall=18, gb_free=8.3, wall=229
2021-09-14 05:34:26 | INFO | train_inner | epoch 005:    182 / 230 loss=0.823, nll_loss=0.015, accuracy=69.3, sample_size=16, f1=0.796388, mcc=0.159271, acc_f1=0.744444, wps=4818.4, ups=5.57, wpb=864.7, bsz=16, num_updates=1100, lr=9.00621e-05, gnorm=19.613, loss_scale=128, train_wall=18, gb_free=8.4, wall=247
2021-09-14 05:34:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:34:35 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.716 | nll_loss 0.013 | accuracy 76.5 | sample_size 15.6923 | f1 0.810031 | mcc 0.490838 | acc_f1 0.787227 | wps 20927.2 | wpb 851.3 | bsz 15.7 | num_updates 1148 | best_accuracy 76.5
2021-09-14 05:34:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1148 updates
2021-09-14 05:34:35 | INFO | fairseq.trainer | Saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:34:45 | INFO | fairseq.trainer | Finished saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:34:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint tmp/out/checkpoint_best.pt (epoch 5 @ 1148 updates, score 76.5) (writing took 10.042264614254236 seconds)
2021-09-14 05:34:45 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-09-14 05:34:45 | INFO | train | epoch 005 | loss 0.819 | nll_loss 0.015 | accuracy 70.4 | sample_size 15.9476 | f1 0.800634 | mcc 0.19721 | acc_f1 0.751982 | wps 3742.6 | ups 4.34 | wpb 862.6 | bsz 15.9 | num_updates 1148 | lr 8.93168e-05 | gnorm 18.442 | loss_scale 256 | train_wall 41 | gb_free 8.5 | wall 266
2021-09-14 05:34:45 | INFO | fairseq.trainer | begin training epoch 6
2021-09-14 05:34:45 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:34:55 | INFO | train_inner | epoch 006:     52 / 230 loss=0.755, nll_loss=0.014, accuracy=73.9, sample_size=16, f1=0.805179, mcc=0.389659, acc_f1=0.771964, wps=2973.2, ups=3.42, wpb=868.1, bsz=16, num_updates=1200, lr=8.85093e-05, gnorm=22.693, loss_scale=256, train_wall=18, gb_free=8.4, wall=276
2021-09-14 05:35:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2021-09-14 05:35:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2021-09-14 05:35:13 | INFO | train_inner | epoch 006:    154 / 230 loss=0.673, nll_loss=0.012, accuracy=78.3, sample_size=15.88, f1=0.831761, mcc=0.513185, acc_f1=0.807443, wps=4698.1, ups=5.46, wpb=860.2, bsz=15.9, num_updates=1300, lr=8.69565e-05, gnorm=18.147, loss_scale=128, train_wall=18, gb_free=8.4, wall=294
2021-09-14 05:35:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-09-14 05:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:35:28 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.619 | nll_loss 0.011 | accuracy 83.1 | sample_size 15.6923 | f1 0.869833 | mcc 0.62393 | acc_f1 0.850782 | wps 21577.9 | wpb 851.3 | bsz 15.7 | num_updates 1375 | best_accuracy 83.1
2021-09-14 05:35:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1375 updates
2021-09-14 05:35:28 | INFO | fairseq.trainer | Saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:35:38 | INFO | fairseq.trainer | Finished saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:35:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint tmp/out/checkpoint_best.pt (epoch 6 @ 1375 updates, score 83.1) (writing took 10.056727282702923 seconds)
2021-09-14 05:35:38 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-09-14 05:35:38 | INFO | train | epoch 006 | loss 0.687 | nll_loss 0.013 | accuracy 77.4 | sample_size 15.9471 | f1 0.826882 | mcc 0.487862 | acc_f1 0.800418 | wps 3742.5 | ups 4.34 | wpb 862.5 | bsz 15.9 | num_updates 1375 | lr 8.57919e-05 | gnorm 23.156 | loss_scale 64 | train_wall 40 | gb_free 8.6 | wall 319
2021-09-14 05:35:38 | INFO | fairseq.trainer | begin training epoch 7
2021-09-14 05:35:38 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:35:42 | INFO | train_inner | epoch 007:     25 / 230 loss=0.663, nll_loss=0.012, accuracy=78.9, sample_size=16, f1=0.843509, mcc=0.50737, acc_f1=0.81613, wps=2958.5, ups=3.43, wpb=861.4, bsz=16, num_updates=1400, lr=8.54037e-05, gnorm=30.797, loss_scale=64, train_wall=18, gb_free=8.1, wall=323
2021-09-14 05:36:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-09-14 05:36:00 | INFO | train_inner | epoch 007:    126 / 230 loss=0.787, nll_loss=0.015, accuracy=75.3, sample_size=15.88, f1=0.828967, mcc=0.2978, acc_f1=0.791046, wps=4688.1, ups=5.47, wpb=857.2, bsz=15.9, num_updates=1500, lr=8.38509e-05, gnorm=32.714, loss_scale=64, train_wall=18, gb_free=8.5, wall=342
2021-09-14 05:36:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-09-14 05:36:18 | INFO | train_inner | epoch 007:    227 / 230 loss=0.78, nll_loss=0.014, accuracy=73.4, sample_size=16, f1=0.810843, mcc=0.326963, acc_f1=0.771984, wps=4822.9, ups=5.57, wpb=866, bsz=16, num_updates=1600, lr=8.22981e-05, gnorm=22.519, loss_scale=32, train_wall=18, gb_free=8.3, wall=360
2021-09-14 05:36:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:36:20 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.578 | nll_loss 0.011 | accuracy 82.1 | sample_size 15.6923 | f1 0.873593 | mcc 0.560077 | acc_f1 0.846652 | wps 21639.8 | wpb 851.3 | bsz 15.7 | num_updates 1603 | best_accuracy 83.1
2021-09-14 05:36:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1603 updates
2021-09-14 05:36:20 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-09-14 05:36:20 | INFO | train | epoch 007 | loss 0.766 | nll_loss 0.014 | accuracy 75.1 | sample_size 15.9474 | f1 0.825049 | mcc 0.335 | acc_f1 0.787662 | wps 4634 | ups 5.37 | wpb 862.3 | bsz 15.9 | num_updates 1603 | lr 8.22516e-05 | gnorm 28.726 | loss_scale 32 | train_wall 41 | gb_free 8.4 | wall 361
2021-09-14 05:36:20 | INFO | fairseq.trainer | begin training epoch 8
2021-09-14 05:36:20 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:36:37 | INFO | train_inner | epoch 008:     97 / 230 loss=0.631, nll_loss=0.012, accuracy=81.4, sample_size=16, f1=0.86405, mcc=0.551266, acc_f1=0.8389, wps=4576, ups=5.29, wpb=865.4, bsz=16, num_updates=1700, lr=8.07453e-05, gnorm=29.657, loss_scale=32, train_wall=18, gb_free=8.3, wall=378
2021-09-14 05:36:55 | INFO | train_inner | epoch 008:    197 / 230 loss=0.588, nll_loss=0.011, accuracy=82.8, sample_size=16, f1=0.861801, mcc=0.648189, acc_f1=0.844963, wps=4898.7, ups=5.65, wpb=866.9, bsz=16, num_updates=1800, lr=7.91925e-05, gnorm=20.105, loss_scale=64, train_wall=17, gb_free=8.3, wall=396
2021-09-14 05:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:37:02 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.596 | nll_loss 0.011 | accuracy 81.6 | sample_size 15.6923 | f1 0.844773 | mcc 0.663359 | acc_f1 0.83104 | wps 21528.9 | wpb 851.3 | bsz 15.7 | num_updates 1833 | best_accuracy 83.1
2021-09-14 05:37:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1833 updates
2021-09-14 05:37:02 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-09-14 05:37:02 | INFO | train | epoch 008 | loss 0.608 | nll_loss 0.011 | accuracy 82.1 | sample_size 15.9478 | f1 0.860862 | mcc 0.601571 | acc_f1 0.840893 | wps 4733.6 | ups 5.49 | wpb 862.5 | bsz 15.9 | num_updates 1833 | lr 7.86801e-05 | gnorm 24.283 | loss_scale 64 | train_wall 40 | gb_free 7.7 | wall 403
2021-09-14 05:37:02 | INFO | fairseq.trainer | begin training epoch 9
2021-09-14 05:37:02 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:37:14 | INFO | train_inner | epoch 009:     67 / 230 loss=0.534, nll_loss=0.01, accuracy=84.8, sample_size=15.76, f1=0.877604, mcc=0.667821, acc_f1=0.862865, wps=4528.4, ups=5.29, wpb=856.2, bsz=15.8, num_updates=1900, lr=7.76398e-05, gnorm=20.075, loss_scale=128, train_wall=17, gb_free=8.5, wall=415
2021-09-14 05:37:32 | INFO | train_inner | epoch 009:    167 / 230 loss=0.573, nll_loss=0.011, accuracy=83.1, sample_size=16, f1=0.871541, mcc=0.594681, acc_f1=0.851083, wps=4867.9, ups=5.67, wpb=859.1, bsz=16, num_updates=2000, lr=7.6087e-05, gnorm=26.46, loss_scale=256, train_wall=17, gb_free=8.6, wall=433
2021-09-14 05:37:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:37:44 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.515 | nll_loss 0.009 | accuracy 86.5 | sample_size 15.6923 | f1 0.901538 | mcc 0.681678 | acc_f1 0.883461 | wps 21349.2 | wpb 851.3 | bsz 15.7 | num_updates 2063 | best_accuracy 86.5
2021-09-14 05:37:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 2063 updates
2021-09-14 05:37:44 | INFO | fairseq.trainer | Saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:37:54 | INFO | fairseq.trainer | Finished saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:37:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint tmp/out/checkpoint_best.pt (epoch 9 @ 2063 updates, score 86.5) (writing took 10.29361306130886 seconds)
2021-09-14 05:37:54 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-09-14 05:37:54 | INFO | train | epoch 009 | loss 0.57 | nll_loss 0.011 | accuracy 83.3 | sample_size 15.9478 | f1 0.870514 | mcc 0.62459 | acc_f1 0.851969 | wps 3796.8 | ups 4.4 | wpb 862.5 | bsz 15.9 | num_updates 2063 | lr 7.51087e-05 | gnorm 23.806 | loss_scale 256 | train_wall 40 | gb_free 8.5 | wall 455
2021-09-14 05:37:54 | INFO | fairseq.trainer | begin training epoch 10
2021-09-14 05:37:54 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:37:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2021-09-14 05:38:02 | INFO | train_inner | epoch 010:     38 / 230 loss=0.599, nll_loss=0.011, accuracy=81.8, sample_size=16, f1=0.854622, mcc=0.603195, acc_f1=0.836061, wps=2901.1, ups=3.33, wpb=871.8, bsz=16, num_updates=2100, lr=7.45342e-05, gnorm=24.028, loss_scale=128, train_wall=18, gb_free=8.3, wall=463
2021-09-14 05:38:21 | INFO | train_inner | epoch 010:    138 / 230 loss=0.536, nll_loss=0.01, accuracy=85.1, sample_size=16, f1=0.888875, mcc=0.654885, acc_f1=0.86975, wps=4363.8, ups=5.04, wpb=865.5, bsz=16, num_updates=2200, lr=7.29814e-05, gnorm=25.865, loss_scale=256, train_wall=19, gb_free=8.6, wall=483
2021-09-14 05:38:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2021-09-14 05:38:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:38:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.405 | nll_loss 0.007 | accuracy 88.7 | sample_size 15.6923 | f1 0.913107 | mcc 0.736323 | acc_f1 0.900063 | wps 21147.2 | wpb 851.3 | bsz 15.7 | num_updates 2291 | best_accuracy 88.7
2021-09-14 05:38:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 2291 updates
2021-09-14 05:38:40 | INFO | fairseq.trainer | Saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:38:50 | INFO | fairseq.trainer | Finished saving checkpoint to tmp/out/checkpoint_best.pt
2021-09-14 05:38:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint tmp/out/checkpoint_best.pt (epoch 10 @ 2291 updates, score 88.7) (writing took 10.088098872452974 seconds)
2021-09-14 05:38:50 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-09-14 05:38:50 | INFO | train | epoch 010 | loss 0.518 | nll_loss 0.01 | accuracy 85.3 | sample_size 15.9474 | f1 0.88695 | mcc 0.665604 | acc_f1 0.870421 | wps 3539.5 | ups 4.11 | wpb 862 | bsz 15.9 | num_updates 2291 | lr 7.15683e-05 | gnorm 23.726 | loss_scale 128 | train_wall 44 | gb_free 8.5 | wall 511
2021-09-14 05:38:50 | INFO | fairseq.trainer | begin training epoch 11
2021-09-14 05:38:50 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:38:51 | INFO | train_inner | epoch 011:      9 / 230 loss=0.498, nll_loss=0.009, accuracy=86.5, sample_size=15.88, f1=0.891759, mcc=0.699919, acc_f1=0.878692, wps=2854.7, ups=3.34, wpb=855, bsz=15.9, num_updates=2300, lr=7.14286e-05, gnorm=23.361, loss_scale=128, train_wall=18, gb_free=8.6, wall=513
2021-09-14 05:39:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2021-09-14 05:39:10 | INFO | train_inner | epoch 011:    110 / 230 loss=0.491, nll_loss=0.009, accuracy=86.6, sample_size=15.88, f1=0.895704, mcc=0.701331, acc_f1=0.881289, wps=4626.1, ups=5.43, wpb=852.3, bsz=15.9, num_updates=2400, lr=6.98758e-05, gnorm=30.153, loss_scale=128, train_wall=18, gb_free=8.3, wall=531
2021-09-14 05:39:28 | INFO | train_inner | epoch 011:    210 / 230 loss=0.472, nll_loss=0.009, accuracy=87.1, sample_size=16, f1=0.902079, mcc=0.705297, acc_f1=0.886665, wps=4849.7, ups=5.57, wpb=871.2, bsz=16, num_updates=2500, lr=6.8323e-05, gnorm=25.557, loss_scale=128, train_wall=18, gb_free=8.6, wall=549
2021-09-14 05:39:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:39:32 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.413 | nll_loss 0.008 | accuracy 88 | sample_size 15.6923 | f1 0.907582 | mcc 0.724256 | acc_f1 0.893695 | wps 21504.3 | wpb 851.3 | bsz 15.7 | num_updates 2520 | best_accuracy 88.7
2021-09-14 05:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2520 updates
2021-09-14 05:39:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-09-14 05:39:32 | INFO | train | epoch 011 | loss 0.481 | nll_loss 0.009 | accuracy 86.8 | sample_size 15.9476 | f1 0.897871 | mcc 0.703899 | acc_f1 0.883161 | wps 4623.7 | ups 5.36 | wpb 862.3 | bsz 15.9 | num_updates 2520 | lr 6.80124e-05 | gnorm 28.392 | loss_scale 256 | train_wall 41 | gb_free 8.4 | wall 553
2021-09-14 05:39:32 | INFO | fairseq.trainer | begin training epoch 12
2021-09-14 05:39:32 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:39:47 | INFO | train_inner | epoch 012:     80 / 230 loss=0.435, nll_loss=0.008, accuracy=87.7, sample_size=15.88, f1=0.904275, mcc=0.715526, acc_f1=0.88995, wps=4582.9, ups=5.31, wpb=863.1, bsz=15.9, num_updates=2600, lr=6.67702e-05, gnorm=23.395, loss_scale=256, train_wall=17, gb_free=8.3, wall=568
2021-09-14 05:40:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2021-09-14 05:40:05 | INFO | train_inner | epoch 012:    181 / 230 loss=0.442, nll_loss=0.008, accuracy=88.7, sample_size=16, f1=0.912022, mcc=0.728655, acc_f1=0.899449, wps=4811.8, ups=5.56, wpb=864.9, bsz=16, num_updates=2700, lr=6.52174e-05, gnorm=19.898, loss_scale=256, train_wall=18, gb_free=8.4, wall=586
2021-09-14 05:40:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:40:14 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.654 | nll_loss 0.012 | accuracy 82.8 | sample_size 15.6923 | f1 0.87911 | mcc 0.558318 | acc_f1 0.851815 | wps 21218.4 | wpb 851.3 | bsz 15.7 | num_updates 2749 | best_accuracy 88.7
2021-09-14 05:40:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2749 updates
2021-09-14 05:40:14 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-09-14 05:40:14 | INFO | train | epoch 012 | loss 0.445 | nll_loss 0.008 | accuracy 88.1 | sample_size 15.9476 | f1 0.907996 | mcc 0.721182 | acc_f1 0.8945 | wps 4715 | ups 5.47 | wpb 862.4 | bsz 15.9 | num_updates 2749 | lr 6.44565e-05 | gnorm 21.972 | loss_scale 256 | train_wall 40 | gb_free 8.4 | wall 595
2021-09-14 05:40:14 | INFO | fairseq.trainer | begin training epoch 13
2021-09-14 05:40:14 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:40:23 | INFO | train_inner | epoch 013:     51 / 230 loss=0.466, nll_loss=0.009, accuracy=87.7, sample_size=16, f1=0.908585, mcc=0.688698, acc_f1=0.89273, wps=4616.4, ups=5.36, wpb=861.2, bsz=16, num_updates=2800, lr=6.36646e-05, gnorm=28.829, loss_scale=256, train_wall=17, gb_free=8.6, wall=604
2021-09-14 05:40:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2021-09-14 05:40:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2021-09-14 05:40:41 | INFO | train_inner | epoch 013:    153 / 230 loss=0.457, nll_loss=0.008, accuracy=87, sample_size=15.88, f1=0.8981, mcc=0.712244, acc_f1=0.883425, wps=4826.7, ups=5.62, wpb=858.3, bsz=15.9, num_updates=2900, lr=6.21118e-05, gnorm=26.557, loss_scale=128, train_wall=17, gb_free=8.4, wall=622
2021-09-14 05:40:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:40:56 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 0.462 | nll_loss 0.009 | accuracy 88.5 | sample_size 15.6923 | f1 0.911461 | mcc 0.746216 | acc_f1 0.898038 | wps 19681.4 | wpb 851.3 | bsz 15.7 | num_updates 2977 | best_accuracy 88.7
2021-09-14 05:40:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2977 updates
2021-09-14 05:40:56 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-09-14 05:40:56 | INFO | train | epoch 013 | loss 0.453 | nll_loss 0.008 | accuracy 87.4 | sample_size 15.9474 | f1 0.904145 | mcc 0.704556 | acc_f1 0.88875 | wps 4758.4 | ups 5.52 | wpb 862.6 | bsz 15.9 | num_updates 2977 | lr 6.09161e-05 | gnorm 27.346 | loss_scale 256 | train_wall 39 | gb_free 8.3 | wall 637
2021-09-14 05:40:56 | INFO | fairseq.trainer | begin training epoch 14
2021-09-14 05:40:56 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:41:00 | INFO | train_inner | epoch 014:     23 / 230 loss=0.431, nll_loss=0.008, accuracy=88.2, sample_size=15.88, f1=0.910711, mcc=0.730796, acc_f1=0.896606, wps=4532.4, ups=5.27, wpb=859.4, bsz=15.9, num_updates=3000, lr=6.0559e-05, gnorm=23.827, loss_scale=256, train_wall=17, gb_free=8.5, wall=641
2021-09-14 05:41:19 | INFO | train_inner | epoch 014:    123 / 230 loss=0.466, nll_loss=0.009, accuracy=86.8, sample_size=16, f1=0.8986, mcc=0.68423, acc_f1=0.883362, wps=4615.6, ups=5.34, wpb=863.7, bsz=16, num_updates=3100, lr=5.90062e-05, gnorm=29.231, loss_scale=512, train_wall=18, gb_free=8, wall=660
2021-09-14 05:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2021-09-14 05:41:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2021-09-14 05:41:38 | INFO | train_inner | epoch 014:    225 / 230 loss=0.404, nll_loss=0.007, accuracy=89.1, sample_size=16, f1=0.915626, mcc=0.752735, acc_f1=0.903126, wps=4532.4, ups=5.24, wpb=864.9, bsz=16, num_updates=3200, lr=5.74534e-05, gnorm=20.899, loss_scale=128, train_wall=19, gb_free=8.4, wall=679
2021-09-14 05:41:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:41:40 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 0.453 | nll_loss 0.008 | accuracy 88 | sample_size 15.6923 | f1 0.905962 | mcc 0.762026 | acc_f1 0.894087 | wps 20993.6 | wpb 851.3 | bsz 15.7 | num_updates 3205 | best_accuracy 88.7
2021-09-14 05:41:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 3205 updates
2021-09-14 05:41:40 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-09-14 05:41:40 | INFO | train | epoch 014 | loss 0.433 | nll_loss 0.008 | accuracy 88.1 | sample_size 15.9474 | f1 0.908677 | mcc 0.723619 | acc_f1 0.895128 | wps 4442.2 | ups 5.15 | wpb 862.8 | bsz 15.9 | num_updates 3205 | lr 5.73758e-05 | gnorm 25.593 | loss_scale 128 | train_wall 42 | gb_free 8.6 | wall 681
2021-09-14 05:41:40 | INFO | fairseq.trainer | begin training epoch 15
2021-09-14 05:41:40 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:41:57 | INFO | train_inner | epoch 015:     95 / 230 loss=0.389, nll_loss=0.007, accuracy=88.6, sample_size=16, f1=0.909884, mcc=0.752985, acc_f1=0.897754, wps=4474.6, ups=5.21, wpb=859.4, bsz=16, num_updates=3300, lr=5.59006e-05, gnorm=28.048, loss_scale=256, train_wall=18, gb_free=8.4, wall=698
2021-09-14 05:42:14 | INFO | train_inner | epoch 015:    195 / 230 loss=0.427, nll_loss=0.008, accuracy=88.9, sample_size=15.88, f1=0.913506, mcc=0.752861, acc_f1=0.901753, wps=4936, ups=5.73, wpb=862, bsz=15.9, num_updates=3400, lr=5.43478e-05, gnorm=22.95, loss_scale=256, train_wall=17, gb_free=8.4, wall=716
2021-09-14 05:42:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2021-09-14 05:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:42:22 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.429 | nll_loss 0.008 | accuracy 88.5 | sample_size 15.6923 | f1 0.914455 | mcc 0.734955 | acc_f1 0.899535 | wps 21539.7 | wpb 851.3 | bsz 15.7 | num_updates 3434 | best_accuracy 88.7
2021-09-14 05:42:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 3434 updates
2021-09-14 05:42:22 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-09-14 05:42:22 | INFO | train | epoch 015 | loss 0.402 | nll_loss 0.007 | accuracy 89 | sample_size 15.9476 | f1 0.913947 | mcc 0.757894 | acc_f1 0.902115 | wps 4742.5 | ups 5.5 | wpb 862.7 | bsz 15.9 | num_updates 3434 | lr 5.38199e-05 | gnorm 23.451 | loss_scale 256 | train_wall 40 | gb_free 8.5 | wall 723
2021-09-14 05:42:22 | INFO | fairseq.trainer | begin training epoch 16
2021-09-14 05:42:22 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:42:33 | INFO | train_inner | epoch 016:     66 / 230 loss=0.382, nll_loss=0.007, accuracy=90.5, sample_size=15.88, f1=0.929376, mcc=0.781008, acc_f1=0.917501, wps=4646.2, ups=5.39, wpb=861.5, bsz=15.9, num_updates=3500, lr=5.2795e-05, gnorm=20.482, loss_scale=256, train_wall=17, gb_free=8.4, wall=734
2021-09-14 05:42:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2021-09-14 05:42:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-09-14 05:42:51 | INFO | train_inner | epoch 016:    168 / 230 loss=0.382, nll_loss=0.007, accuracy=89.3, sample_size=16, f1=0.915067, mcc=0.761724, acc_f1=0.904096, wps=4901.3, ups=5.65, wpb=866.9, bsz=16, num_updates=3600, lr=5.12422e-05, gnorm=28.126, loss_scale=64, train_wall=17, gb_free=8.3, wall=752
2021-09-14 05:43:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:43:03 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.439 | nll_loss 0.008 | accuracy 87.5 | sample_size 15.6923 | f1 0.908702 | mcc 0.710054 | acc_f1 0.891851 | wps 21521.9 | wpb 851.3 | bsz 15.7 | num_updates 3662 | best_accuracy 88.7
2021-09-14 05:43:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3662 updates
2021-09-14 05:43:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-09-14 05:43:03 | INFO | train | epoch 016 | loss 0.391 | nll_loss 0.007 | accuracy 89.4 | sample_size 15.9474 | f1 0.918541 | mcc 0.754384 | acc_f1 0.906228 | wps 4782.8 | ups 5.54 | wpb 862.7 | bsz 15.9 | num_updates 3662 | lr 5.02795e-05 | gnorm 25.059 | loss_scale 64 | train_wall 39 | gb_free 8.5 | wall 764
2021-09-14 05:43:03 | INFO | fairseq.trainer | begin training epoch 17
2021-09-14 05:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:43:09 | INFO | train_inner | epoch 017:     38 / 230 loss=0.411, nll_loss=0.008, accuracy=88.4, sample_size=16, f1=0.911491, mcc=0.724318, acc_f1=0.897933, wps=4646, ups=5.36, wpb=867.2, bsz=16, num_updates=3700, lr=4.96894e-05, gnorm=21.902, loss_scale=128, train_wall=17, gb_free=8.5, wall=770
2021-09-14 05:43:27 | INFO | train_inner | epoch 017:    138 / 230 loss=0.331, nll_loss=0.006, accuracy=91.6, sample_size=15.88, f1=0.935521, mcc=0.808605, acc_f1=0.926198, wps=4934, ups=5.72, wpb=862.2, bsz=15.9, num_updates=3800, lr=4.81366e-05, gnorm=23.191, loss_scale=256, train_wall=17, gb_free=8.4, wall=788
2021-09-14 05:43:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:43:44 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.389 | nll_loss 0.007 | accuracy 88.5 | sample_size 15.6923 | f1 0.913966 | mcc 0.742465 | acc_f1 0.900493 | wps 21621 | wpb 851.3 | bsz 15.7 | num_updates 3892 | best_accuracy 88.7
2021-09-14 05:43:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3892 updates
2021-09-14 05:43:44 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-09-14 05:43:44 | INFO | train | epoch 017 | loss 0.382 | nll_loss 0.007 | accuracy 89.6 | sample_size 15.9478 | f1 0.921148 | mcc 0.763501 | acc_f1 0.908944 | wps 4815.3 | ups 5.58 | wpb 862.5 | bsz 15.9 | num_updates 3892 | lr 4.67081e-05 | gnorm 22.758 | loss_scale 256 | train_wall 39 | gb_free 8.3 | wall 805
2021-09-14 05:43:44 | INFO | fairseq.trainer | begin training epoch 18
2021-09-14 05:43:44 | INFO | fairseq_cli.train | Start iterating over samples
2021-09-14 05:43:45 | INFO | train_inner | epoch 018:      8 / 230 loss=0.413, nll_loss=0.008, accuracy=88.2, sample_size=16, f1=0.910888, mcc=0.736664, acc_f1=0.896694, wps=4654.7, ups=5.42, wpb=859.4, bsz=16, num_updates=3900, lr=4.65839e-05, gnorm=23.868, loss_scale=256, train_wall=17, gb_free=8.4, wall=806
2021-09-14 05:44:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2021-09-14 05:44:03 | INFO | train_inner | epoch 018:    109 / 230 loss=0.357, nll_loss=0.007, accuracy=91.6, sample_size=15.88, f1=0.93577, mcc=0.811822, acc_f1=0.92601, wps=4886.2, ups=5.69, wpb=858.5, bsz=15.9, num_updates=4000, lr=4.50311e-05, gnorm=23.318, loss_scale=256, train_wall=17, gb_free=8.6, wall=824
2021-09-14 05:44:20 | INFO | train_inner | epoch 018:    209 / 230 loss=0.382, nll_loss=0.007, accuracy=90.1, sample_size=16, f1=0.925988, mcc=0.767418, acc_f1=0.913619, wps=4972.1, ups=5.74, wpb=866.4, bsz=16, num_updates=4100, lr=4.34783e-05, gnorm=22.585, loss_scale=256, train_wall=17, gb_free=8.3, wall=841
2021-09-14 05:44:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-09-14 05:44:25 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.514 | nll_loss 0.009 | accuracy 86.8 | sample_size 15.6923 | f1 0.90326 | mcc 0.689035 | acc_f1 0.884322 | wps 21407.2 | wpb 851.3 | bsz 15.7 | num_updates 4121 | best_accuracy 88.7
2021-09-14 05:44:25 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 8 runs
2021-09-14 05:44:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 4121 updates
2021-09-14 05:44:25 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-09-14 05:44:25 | INFO | train | epoch 018 | loss 0.367 | nll_loss 0.007 | accuracy 90.8 | sample_size 15.9476 | f1 0.92978 | mcc 0.791696 | acc_f1 0.919039 | wps 4798.1 | ups 5.56 | wpb 862.5 | bsz 15.9 | num_updates 4121 | lr 4.31522e-05 | gnorm 23.043 | loss_scale 256 | train_wall 39 | gb_free 8 | wall 846
2021-09-14 05:44:25 | INFO | fairseq_cli.train | done training in 843.5 seconds
